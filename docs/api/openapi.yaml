openapi: 3.0.3
info:
  title: Vertex AI LLM Proxy
  description: |
    A high-performance proxy that bridges OpenAI-compatible clients (like Cursor IDE) to Google Cloud Vertex AI (Gemini).
    It exposes an OpenAI-compatible API interface.
  version: 1.0.0
servers:
  - url: http://localhost:4000
    description: Local Development Server

components:
  securitySchemes:
    BearerAuth:
      type: http
      scheme: bearer
      bearerFormat: API Key

  schemas:
    ChatMessage:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum: [system, user, assistant, tool]
        content:
          oneOf:
            - type: string
            - type: array
              items:
                type: object
                properties:
                  type:
                    type: string
                    enum: [text, image_url]
                  text:
                    type: string
                  image_url:
                    type: object
                    properties:
                      url:
                        type: string
        name:
          type: string

    ChatCompletionRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: The ID of the model to use (e.g., "gemini-flash").
        messages:
          type: array
          items:
            $ref: '#/components/schemas/ChatMessage'
        stream:
          type: boolean
          default: false
        temperature:
          type: number
          default: 1.0
        max_tokens:
          type: integer
        top_p:
          type: number
          default: 1.0

    ChatCompletionResponse:
      type: object
      properties:
        id:
          type: string
        object:
          type: string
          enum: [chat.completion]
        created:
          type: integer
        model:
          type: string
        choices:
          type: array
          items:
            type: object
            properties:
              index:
                type: integer
              message:
                $ref: '#/components/schemas/ChatMessage'
              finish_reason:
                type: string
        usage:
          type: object
          properties:
            prompt_tokens:
              type: integer
            completion_tokens:
              type: integer
            total_tokens:
              type: integer

paths:
  /health:
    get:
      summary: Health Check
      description: Returns the health status of the proxy.
      responses:
        '200':
          description: Healthy
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    example: "ok"
                  version:
                    type: string

  /v1/chat/completions:
    post:
      summary: Create Chat Completion
      description: Generates a model response for the given chat conversation.
      security:
        - BearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatCompletionRequest'
      responses:
        '200':
          description: Successful response (JSON or SSE Stream)
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletionResponse'
            text/event-stream:
              schema:
                type: string
                description: Server-Sent Events stream of ChatCompletionChunk
        '401':
          description: Unauthorized (Invalid API Key)
        '429':
          description: Rate Limit Exceeded
        '500':
          description: Internal Server Error (Upstream Provider Failure)

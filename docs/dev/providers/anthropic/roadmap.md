# Roadmap: Multi-Provider LLM Proxy

> **Manifesto**: Build a unified, high-performance bridge connecting OpenAI-compatible clients to multiple LLM providers (Vertex AI, Anthropic CLI, DeepSeek, Ollama).

## ğŸ† Victory Conditions (Definition of Done)

**Primary**: A user can point Cursor IDE to `http://localhost:4000/v1` and successfully chat with any supported model (`gemini-*`, `claude-*`, `deepseek-*`, `ollama-*`).

**Success Metrics**:

1. **Multi-Provider Routing**: Model name determines provider automatically.
2. **Anthropic CLI Integration**: `claude-*` models work via stdio bridge (Node.js service).
3. **Reliability**: Token refresh, error handling, circuit breakers.
4. **Performance**: P50 latency overhead < 50ms (Vertex), < 100ms (Anthropic bridge).
5. **Compatibility**: Streaming and non-streaming responses for all providers.

## ğŸ› ï¸ Tech Stack (Locked)

**Core Proxy (Rust)**:

- **Language**: Rust (2024 Edition)
- **Web Framework**: Axum 0.7
- **HTTP Client**: Reqwest (rustls)
- **Serialization**: Serde (JSON)
- **Observability**: Tracing + OpenTelemetry
- **Runtime**: Tokio

**Anthropic Bridge (Node.js/TypeScript)**:

- **Language**: TypeScript
- **Runtime**: Node.js
- **Framework**: Express
- **ANSI Stripping**: strip-ansi
- **Process Management**: child_process (spawn)

**No "Resume Driven Development"**: Boring, proven technology only.

## ğŸªœ Phased Execution

### Phase 1: The Skeleton (Core) âœ… COMPLETE

- [x] **Project Init**: `cargo new`, dependency setup (`axum`, `tokio`, `serde`, `reqwest`).
- [x] **Configuration**: Implement `config-rs` to load config from TOML and env vars.
- [x] **Health Check**: `GET /health` endpoint.
- [x] **Auth Middleware**: Validate `Authorization: Bearer <sk-...>` against config.
- [x] **Logging**: Setup `tracing-subscriber` for structured logs.

### Phase 2: The Bridge (Feature) âœ… VERTEX AI COMPLETE

**Vertex AI (Done)**:

- [x] **Type Definitions**: Rust structs for OpenAI Request/Response and Vertex Request/Response.
- [x] **Google Auth**: `TokenManager` to fetch/refresh Google OAuth2 tokens (Service Account/ADC).
- [x] **Translation Layer**:
  - [x] `OpenAI -> Vertex`: Convert messages, temperature, max_tokens.
  - [x] `Vertex -> OpenAI`: Convert candidates, usage metadata.
- [x] **Proxy Handler**: `POST /v1/chat/completions`.
  - [x] Unary (Non-streaming) support.
  - [x] Streaming (SSE) support.

**Anthropic CLI Bridge (âœ… COMPLETE)**:

- [x] **Provider Router**: Model name â†’ provider mapping (`claude-*` â†’ AnthropicCLI).
- [x] **Anthropic Bridge Service**: Node.js/TypeScript stdio-to-HTTP bridge.
  - [x] Express server on internal port (4001).
  - [x] `POST /anthropic/chat` endpoint.
  - [x] Message concatenation (`messages[]` â†’ prompt string).
  - [x] `spawn('claude', ['-p', prompt])` with ANSI stripping.
  - [x] SSE chunk wrapping (OpenAI format).
- [x] **Rust Integration**: HTTP client to call Anthropic bridge.
  - [x] `reqwest` client for internal bridge communication.
  - [x] Stream forwarding (bridge SSE â†’ proxy SSE).
- [x] **Error Handling**: CLI errors (stderr) â†’ OpenAI error format.

**Provider Abstraction (âœ… COMPLETE)**:

- [x] **Provider Trait**: Define `Provider` trait with `execute()` method.
- [x] **Provider Registry**: Map model names to provider instances.
- [x] **Router Middleware**: Route requests based on model name.

### Phase 3: Resilience & Polish

- [ ] **Error Handling**: Map provider errors (400, 401, 429) to OpenAI-compatible responses.
- [ ] **Rate Limiting**: In-memory token bucket per IP/API key (governor crate).
- [ ] **Circuit Breaker**: Health state tracking per provider.
- [ ] **Fallback Logic**: Automatic provider switching on failure (Vertex â†’ DeepSeek â†’ Ollama).
- [ ] **Timeout Handling**: Request timeouts per provider (different for CLI vs HTTP).
- [ ] **Integration Tests**: Test against real providers (mocked and live).

### Phase 4: Ship

- [ ] **Docker**: Multi-stage `Dockerfile` (Rust proxy + Node.js bridge).
  - [ ] Rust binary (distroless/cc base).
  - [ ] Node.js bridge (alpine base).
  - [ ] `docker-compose.yml` for local development.
- [ ] **Release**: Binary builds for macOS/Linux (Rust proxy).
- [ ] **Documentation**:
  - [x] System design (`docs/anthropic/system-design.md`).
  - [x] API contract (`docs/api/api-contract.ts`).
  - [ ] User guide for Cursor/VSCode setup.
  - [ ] Provider configuration guide.
- [ ] **CI/CD**: GitHub Actions for tests, builds, releases.

## ğŸ“‚ Directory Structure

```text
.
â”œâ”€â”€ src/                    # Rust proxy core
â”‚   â”œâ”€â”€ config/             # Configuration loading
â”‚   â”œâ”€â”€ handlers/          # API Route Handlers
â”‚   â”‚   â”œâ”€â”€ chat.rs        # Main chat completions handler
â”‚   â”‚   â””â”€â”€ health.rs      # Health check
â”‚   â”œâ”€â”€ models/            # Request/Response Structs
â”‚   â”‚   â”œâ”€â”€ openai.rs      # OpenAI format
â”‚   â”‚   â””â”€â”€ vertex.rs      # Vertex format
â”‚   â”œâ”€â”€ services/          # Business Logic
â”‚   â”‚   â”œâ”€â”€ auth.rs        # Token management
â”‚   â”‚   â”œâ”€â”€ transformer.rs # Format conversion
â”‚   â”‚   â””â”€â”€ providers/     # Provider implementations
â”‚   â”‚       â”œâ”€â”€ mod.rs     # Provider registry and routing logic
â”‚   â”‚       â”œâ”€â”€ vertex.rs
â”‚   â”‚       â””â”€â”€ anthropic.rs
â”‚   â”œâ”€â”€ middleware/         # Auth, Logging, Rate Limiting
â”‚   â””â”€â”€ utils/             # Helpers
â”œâ”€â”€ bridge/                # Anthropic CLI bridge (NEW)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ index.ts       # Express server
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”œâ”€â”€ tests/                  # Integration Tests
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ anthropic/
â”‚   â”œâ”€â”€ openai/
â”‚   â””â”€â”€ api/
â””â”€â”€ infra/                  # Docker, K8s
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ docker-compose.yml
```

## ğŸ¯ Immediate Next Steps

1. **Create Provider Abstraction**:
   - Define `Provider` trait in `src/services/providers/mod.rs`.
   - Implement `VertexProvider` (refactor existing code).
   - Create `AnthropicBridgeProvider` (HTTP client to bridge).

2. **Build Anthropic Bridge**:
   - Initialize Node.js project in `bridge/`.
   - Implement Express server with `/anthropic/chat` endpoint.
   - Add stdio capture and ANSI stripping.

3. **Integrate Router**:
   - Update `chat.rs` handler to use provider router.
   - Route by model name prefix.

4. **Test End-to-End**:
   - Verify `claude-3-5-sonnet` works via bridge.
   - Verify `gemini-2.0-flash` still works via Vertex.

---

**Status**: Phase 1 & 2 (Vertex AI & Anthropic CLI Bridge) complete. Phase 3 (Resilience & Polish) in progress.
